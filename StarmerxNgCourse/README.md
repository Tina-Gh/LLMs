# Attention in Transformers: Concepts and Code in PyTorch:

<a href="https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/" target="_blank">Link</a>

## Attention and Self-Attention:

<img src="Transformers&Attention_PyTorch_Self-Attention_StarmerxNgCourse.png" alt="Attention and self-attention">
<hr>

## Masked Self-Attention:

<img src="Transformers&Attention_PyTorch_MaskedSelf-Attention_StarmerxNgCourse.png" alt="Masked self-attention">
<hr>

## Encoder Decoder Attention: 

<img src="EncoderDecoderAttention_MultiModal.png" alt="EncoderDecoder">
<hr> 

## Multi-Head Attention:

<img src="multiHeadAttention.png" alt="MultiHead">


